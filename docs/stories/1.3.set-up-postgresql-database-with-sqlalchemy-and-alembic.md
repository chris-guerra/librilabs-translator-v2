# Story 1.3: Set Up PostgreSQL Database with SQLAlchemy and Alembic

## Status
Draft

## Story

**As a** developer,  
**I want** PostgreSQL database connection with SQLAlchemy ORM and Alembic migrations configured,  
**so that** I can store and manage application data with proper schema versioning.

## Acceptance Criteria

1. PostgreSQL database is accessible via Docker Compose (local development) or Railway managed Postgres (production)
2. SQLAlchemy 2.0 is configured with asyncpg driver and AsyncSession
3. Database connection pooling is configured
4. Alembic is set up for database migrations
5. Initial migration creates base schema structure
6. Database connection can be established and tested
7. Environment variables for database configuration are properly managed

## Tasks / Subtasks

- [ ] Task 1: Install Database Dependencies (AC: 2)
  - [ ] Add SQLAlchemy 2.0 to `backend/requirements.txt`
  - [ ] Add asyncpg driver to `backend/requirements.txt`
  - [ ] Add Alembic to `backend/requirements.txt`
  - [ ] Install dependencies: `pip install -r requirements.txt` (or via Docker if using containerized backend)
  - [ ] Verify SQLAlchemy 2.0 and asyncpg are installed correctly

- [ ] Task 2: Configure Database Connection (AC: 1, 2, 3, 7)
  - [ ] Create `backend/app/database.py` with AsyncSession setup
  - [ ] Configure SQLAlchemy engine with asyncpg driver
  - [ ] Set up connection pooling (pool_size, max_overflow)
  - [ ] Create `get_db` dependency function for FastAPI
  - [ ] Add DATABASE_URL environment variable to `backend/.env.example`
  - [ ] Update `backend/app/config.py` to include database configuration
  - [ ] Document database connection string format in README
  - [ ] **Note:** DATABASE_URL should work with docker-compose PostgreSQL service
  - [ ] Document connection string format for containerized environment: `postgresql+asyncpg://librilabs:librilabs_dev@postgres:5432/librilabs_translator`
  - [ ] Document connection string format for Railway: Use Railway's DATABASE_URL environment variable

- [ ] Task 3: Set Up Alembic for Migrations (AC: 4, 5)
  - [ ] Initialize Alembic: `alembic init alembic` in backend directory
  - [ ] Configure `alembic/env.py` to use AsyncSession and async engine
  - [ ] Update `alembic.ini` with database URL from environment variables
  - [ ] Create initial migration: `alembic revision --autogenerate -m "Initial schema"`
  - [ ] Review and adjust migration to create base schema (users, documents, translations tables)
  - [ ] Apply migration: `alembic upgrade head`
  - [ ] Verify tables are created in database
  - [ ] **Note:** If using Docker, run migrations from within backend container or via `make` command

- [ ] Task 4: Create Base Schema Models (AC: 5)
  - [ ] Create `backend/app/models/__init__.py`
  - [ ] Create `backend/app/models/user.py` with User model (for post-MVP)
  - [ ] Create `backend/app/models/document.py` with Document model
  - [ ] Create `backend/app/models/translation.py` with Translation model
  - [ ] Ensure models match database schema from architecture docs
  - [ ] Add proper relationships and constraints to models

- [ ] Task 5: Test Database Connection (AC: 6)
  - [ ] Create test script or test case to verify database connection
  - [ ] Test AsyncSession creation and basic query
  - [ ] Verify connection pooling works correctly
  - [ ] Test database connection from FastAPI application context
  - [ ] Test database connection from containerized backend (if using Docker)
  - [ ] Document how to test database connection manually
  - [ ] **Note:** Database will be containerized if using Docker Compose from Story 1.2

- [ ] Task 6: Update Configuration and Documentation (AC: 7)
  - [ ] Update `backend/.env.example` with DATABASE_URL placeholder
  - [ ] Document database setup in `backend/README.md`
  - [ ] Include instructions for local PostgreSQL setup (Docker Compose)
  - [ ] Include instructions for Railway PostgreSQL setup
  - [ ] Document Alembic migration commands
  - [ ] Document how to run migrations in Docker environment
  - [ ] Add database connection troubleshooting section

## Dev Notes

### Previous Story Insights

From Story 1.2 (Set Up Docker and Deployment Configuration):
- Docker Compose provides PostgreSQL service for local development
- Database connection should work with containerized PostgreSQL from docker-compose.yml
- Use `make up` or `docker-compose up` to start database service
- PostgreSQL service is available at `postgres:5432` from within Docker network
- Connection string format: `postgresql+asyncpg://librilabs:librilabs_dev@postgres:5432/librilabs_translator`
- Environment variables are configured via .env files and docker-compose.yml

From Story 1.1 (Set Up FastAPI Backend Structure with Health Check):
- FastAPI application structure is established in `backend/app/` directory
- Configuration management is handled via `backend/app/config.py` using Pydantic BaseSettings
- Environment variables are loaded from `.env` file (not hardcoded)
- `.env.example` file pattern is used for documenting required environment variables
- Testing framework (Pytest) is already configured and working
- httpx test client is available for API endpoint testing

**Relevant Patterns:**
- Configuration follows Pydantic BaseSettings pattern for type-safe environment variable access
- Error handling uses FastAPI's HTTPException with structured error responses
- Dependencies are managed via FastAPI's Depends() system

### Data Models
[Source: architecture/data-models.md]

**Document Model:**
- Fields: id (UUID), content (TEXT), file_name (VARCHAR(255)), file_size (INTEGER), source_language (VARCHAR(10)), user_id (UUID, nullable), session_id (VARCHAR(255), nullable), created_at (TIMESTAMP), updated_at (TIMESTAMP)
- Relationships: One Document has many Translations (one-to-many)
- Constraints: File size max 10MB (10485760 bytes), either user_id or session_id must be present

**Translation Model:**
- Fields: id (UUID), document_id (UUID, FK), target_language (VARCHAR(10)), translated_content (TEXT, nullable), status (VARCHAR(20)), progress_percentage (INTEGER 0-100), translation_state (JSONB, nullable), user_id (UUID, nullable), session_id (VARCHAR(255), nullable), created_at (TIMESTAMP), updated_at (TIMESTAMP)
- Relationships: One Translation belongs to one Document (many-to-one)
- Constraints: Unique constraint on (document_id, target_language), status enum: 'pending', 'in_progress', 'completed', 'failed', either user_id or session_id must be present

**User Model (Post-MVP):**
- Fields: id (UUID), email (VARCHAR(255), unique), created_at (TIMESTAMP), updated_at (TIMESTAMP)
- Note: User model is created in schema but not actively used during MVP (user_id fields are nullable)

### Database Schema
[Source: architecture/database-schema.md]

**Schema Structure:**
- UUID extension enabled: `CREATE EXTENSION IF NOT EXISTS "uuid-ossp"`
- Users table: id (UUID PK), email (VARCHAR(255) UNIQUE), created_at, updated_at
- Documents table: id (UUID PK), content (TEXT), file_name (VARCHAR(255)), file_size (INTEGER with CHECK constraint 0-10485760), source_language (VARCHAR(10)), user_id (UUID FK, nullable), session_id (VARCHAR(255)), created_at, updated_at
- Translations table: id (UUID PK), document_id (UUID FK), target_language (VARCHAR(10)), translated_content (TEXT), status (VARCHAR(20) with CHECK constraint), progress_percentage (INTEGER 0-100), translation_state (JSONB), user_id (UUID FK, nullable), session_id (VARCHAR(255)), created_at, updated_at
- Unique constraint: `unique_document_target_language` on (document_id, target_language)
- Check constraints: file_size validation, status enum validation, progress_percentage range validation
- Indexes: Foreign key indexes, session_id indexes, status indexes, created_at indexes, full-text search indexes (GIN)

**Schema Features:**
- Automatic updated_at triggers using `update_updated_at_column()` function
- Cascade deletes on foreign keys (ON DELETE CASCADE)
- Full-text search indexes prepared for future search functionality

### API Specifications
[Source: architecture/backend-architecture.md]

**Database Connection Pattern:**
- Use SQLAlchemy AsyncSession for all database operations
- Database connection managed via FastAPI dependency: `get_db() -> AsyncSession`
- Connection pooling configured at engine level
- Repository pattern used for data access (to be implemented in later stories)

**File Locations:**
- Database connection: `backend/app/database.py`
- SQLAlchemy models: `backend/app/models/`
- Alembic migrations: `backend/alembic/versions/`
- Alembic configuration: `backend/alembic/env.py` and `backend/alembic.ini`

### Component Specifications
[Source: architecture/unified-project-structure.md]

**Backend Project Structure:**
```
backend/
├── app/
│   ├── models/          # SQLAlchemy models
│   ├── database.py     # Database connection and session
│   └── config.py       # Configuration management
├── alembic/            # Database migrations
│   ├── versions/       # Migration files
│   └── env.py         # Alembic environment configuration
└── alembic.ini         # Alembic configuration file
```

**Database Connection String Format:**
- Local (Docker Compose): `postgresql+asyncpg://librilabs:librilabs_dev@postgres:5432/librilabs_translator`
- Railway: `postgresql+asyncpg://user:password@host:port/dbname` (from Railway DATABASE_URL)

### File Locations
[Source: architecture/unified-project-structure.md]

**New Files to Create:**
- `backend/app/database.py` - Database connection and AsyncSession setup
- `backend/app/models/__init__.py` - Models package initialization
- `backend/app/models/user.py` - User SQLAlchemy model
- `backend/app/models/document.py` - Document SQLAlchemy model
- `backend/app/models/translation.py` - Translation SQLAlchemy model
- `backend/alembic/env.py` - Alembic environment configuration (auto-generated, then modified)
- `backend/alembic.ini` - Alembic configuration (auto-generated, then modified)
- `backend/alembic/versions/` - Migration files directory (auto-generated)

**Files to Modify:**
- `backend/app/config.py` - Add database configuration settings
- `backend/requirements.txt` - Add SQLAlchemy, asyncpg, Alembic dependencies
- `backend/.env.example` - Add DATABASE_URL environment variable
- `backend/README.md` - Add database setup documentation

### Testing Requirements
[Source: architecture/testing-strategy.md]

**Backend Testing Standards:**
- Test file location: `backend/tests/unit/` for unit tests, `backend/tests/integration/` for integration tests
- Testing framework: Pytest with pytest-asyncio for async tests
- Test database: Use separate test database or test fixtures with transaction rollback
- Database testing pattern: Use AsyncSession fixtures for database operations in tests

**Testing Requirements for This Story:**
- Unit test for database connection establishment
- Integration test for AsyncSession creation and basic query
- Test for connection pooling behavior
- Test for Alembic migration application
- **Note:** If using Docker, tests should work with containerized database

### Technical Constraints
[Source: architecture/tech-stack.md]

**Technology Versions:**
- SQLAlchemy: 2.0 (must use AsyncSession, not legacy Session)
- Database Driver: asyncpg (Latest) - High-performance async PostgreSQL driver
- Database Migrations: Alembic (Latest) - Standard migration tool for SQLAlchemy
- Python: 3.14 - Backend language version
- PostgreSQL: 16 (Docker Compose) or Latest (Railway managed) - Database version

**Technical Requirements:**
- Must use SQLAlchemy 2.0 with AsyncSession (not legacy Session)
- Must use asyncpg driver (not psycopg2)
- Connection pooling must be configured for performance
- Alembic must be configured to work with async engine and AsyncSession
- Environment variables must be used for database configuration (never hardcoded)
- Database connection must work with Docker Compose PostgreSQL service (local development)

**Database Configuration:**
- Connection string format: `postgresql+asyncpg://user:password@host:port/dbname`
- Connection pooling: Configure pool_size and max_overflow for optimal performance
- Timezone: Use TIMESTAMP WITH TIME ZONE for all timestamp fields
- Docker Compose service name: `postgres` (accessible at `postgres:5432` from Docker network)

### Project Structure Notes
[Source: architecture/unified-project-structure.md]

**Alignment with Project Structure:**
- Database connection file location matches structure: `backend/app/database.py`
- Models location matches structure: `backend/app/models/`
- Alembic location matches structure: `backend/alembic/`
- Configuration management follows established pattern from Story 1.1

**No structural conflicts identified** - All file locations align with unified project structure.

### Testing

**Test File Location:**
- Unit tests: `backend/tests/unit/test_database.py`
- Integration tests: `backend/tests/integration/test_database.py`

**Test Standards:**
- Use Pytest with pytest-asyncio for async database tests
- Use test fixtures for database session management
- Test database connection establishment
- Test AsyncSession creation and basic queries
- Test connection pooling behavior
- Test Alembic migration application
- **Note:** If using Docker Compose, database will be containerized

**Testing Frameworks:**
- Pytest (Latest) - Unit/integration testing framework
- pytest-asyncio - For async test support
- httpx - Already available for API testing (not needed for this story)

**Specific Testing Requirements:**
- Verify database connection can be established
- Verify AsyncSession works correctly with async queries
- Verify connection pooling is configured and working
- Verify Alembic migrations can be created and applied
- Verify base schema tables are created correctly
- Verify database connection works from containerized backend (if using Docker)

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-01-27 | 1.0 | Story created (renumbered from Story 1.2 to Story 1.3, updated with Docker references) | John (PM) |

## Dev Agent Record

This section is populated by the development agent during implementation.

### Agent Model Used
_To be filled by dev agent_

### Debug Log References
_To be filled by dev agent_

### Completion Notes List
_To be filled by dev agent_

### File List
_To be filled by dev agent_

## QA Results
_Results from QA Agent review of the completed story implementation_

